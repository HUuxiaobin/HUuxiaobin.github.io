<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xiaobin Hu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiaobin Hu (胡晓彬)</name>
              </p>
              <p>I am a fourth Ph.D Candidate at <a href="https://www.tum.de/">Technische Universität München, Germany </a> CAMP (IBBM) Group, supervised by Prof. Bjoern Menze.
                 Before that, I obtained my master at <a href="https://www.hnu.edu.cn/index.htm">Hunan University </a>, where I studied on the physics-based FEM probability analysis and optimization.
              </p>
              <p>
               As a <B> first </B> or <B> corrspoding </B> author, I have published over <B> 12 </B> papers on top-tier conferences and journals, such as ICCV, ECCV, TPAMI, MICCAI, JBHI, EJNM etc.
              </p>
		
           </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="./files/photo.jpg"><img style="width:100%;max-width:100%;border-radius:100%" alt="Avatar" src="./files/photo.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
	</tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
		    
	My research interests are mainly at image restoration and neural architecture search.
	Currently, I investigate on how to solve <B> the real-world low-quality image restoration </B> problems. In addition, I also have some experience on the following research topics:</p>
	<ul>
	<li>Image restoration inculding super-resolution and deblurring tasks</li>
	<li>Neural architecture search</li>	
	<li>Image to image translation</li>
	<li>3D face and related Tasks</li>
	<li>Segmentation, detection and classification</li>
	<li>Domain adaptation</li>
	
<!-- 	<li>GAN Priors</li> -->
<!-- 	<li>Image Inpainting and Completion</li> -->

	</ul>
	  	   
      <p style="text-align:center">
			<a href="mailto:ji.hou@tum.de"><i class="fa fa-envelope" style="font-size:20px" target="_blank"></i>&nbsp; Email</a> &nbsp;/&nbsp;
			<a href="https://scholar.google.de/citations?hl=en&user=63TZXq4AAAAJ" target="_blank"><i class="ai ai-google-scholar-square ai-3x" style="font-size:20px"></i>&nbsp; Google Scholar</a> &nbsp;/&nbsp;
			<a href="https://github.com/sekunde/" target="_blank"><i class="fa fa-github" style="font-size:20px"></i>&nbsp; Github</a> &nbsp;/&nbsp;
			<a href="https://twitter.com/sekunde_" target="_blank"><i class="fa fa-twitter" style="font-size:20px"></i>&nbsp; Twitter</a> &nbsp;/&nbsp;
			<a href="https://www.linkedin.com/in/ji-hou-3b64a2a0/" target="_blank"><i class="fa fa-linkedin" style="font-size:20px"></i>&nbsp; Linkedin</a> 
			</p>	
	         

              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pri3d.jpg" alt="pri3d" width="200" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2104.11225.pdf">
                <papertitle>Pri3D: Can 3D Priors Help 2D Representation Learning?</papertitle>
              </a>
              <br>
              <strong>Ji Hou</strong>,
		<a href="">Saining Xie</a>,
              <a href="">Benjamin Graham</a>,
		<a href="">Angela Dai</a>,
              <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a>
              <br>
		    <em>International Conference on Computer Vision (ICCV), 2021, Montreal, Canada </em>
              <br>
              <a href="https://arxiv.org/pdf/2104.11225.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a>
              <p>Recent advances in 3D perception have shown impressive progress in understanding geometric structures of 3D shapes and even scenes. Inspired by these advances in geometric understanding, we aim to imbue image-based perception with representations learned under geometric constraints.</p>
            </td>
          </tr>
		
		
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/csc.jpg" alt="3dsis" width="185" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2012.09165.pdf">
                <papertitle>Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts</papertitle>
              </a>
              <br>
              <strong>Ji Hou</strong>,
              <a href="">Benjamin Graham</a>,
              <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a>,
	      <a href="">Saining Xie</a>
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR), 2021, Nashville, USA </em><br>
              <font color="red">(Oral Presentation)</font>
              <br>
              <a href="https://arxiv.org/pdf/2012.09165.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=E70xToZLgs4">video</a> /
              <a href="data/csc.txt">bibtex</a> /
              <a href='https://sekunde.github.io/project_efficient/'>project</a> /
	      <a href='https://github.com/facebookresearch/ContrastiveSceneContexts'>code</a>
              <p>Our study reveals that exhaustive labelling of 3D point clouds might be unnecessary; and remarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89% (instance segmentation) and 96% (semantic segmentation) of the baseline performance that uses full annotations.</p>
            </td>
          </tr>
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rfd.jpg" alt="3dsis" width="185" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2011.14744.pdf">
                <papertitle>RfD-Net: Point Scene Understanding by Semantic Instance Reconstruction</papertitle>
              </a>
              <br>
              <a href="">Yinyu Nie</a>,
	      <strong>Ji Hou</strong>,
	      <a href="">Xiaoguang Han</a>,
              <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a>
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR), 2021, Nashville, USA</em> <br>
              <a href="https://arxiv.org/pdf/2011.14744.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=RHHFC2UaZtQ">video</a> /
              <a href="data/rfd.txt">bibtex</a> /
	      <a href="https://github.com/yinyunie/RfDNet">code</a>
              <p>In this work, we introduce RfD-Net that jointly detects and reconstructs dense object surfaces directly from raw point clouds. Instead of representing scenes with regular grids, our method leverages the sparsity of point cloud data and focuses on predicting shapes that are recognized with high objectness.</p>
            </td>
          </tr>

		
		
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/revealnet.png" alt="revealnet" width="160" style="border-radius:15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1904.12012.pdf">
                <papertitle>RevealNet: Seeing Behind Objects in RGB-D Scans</papertitle>
              </a>
              <br>
              <strong>Ji Hou</strong>,
              <a href="https://www.3dunderstanding.org/team.html">Angela Dai</a>,
              <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a>
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR), 2020, Seattle, USA</em>
              <br>
              <a href="https://arxiv.org/pdf/1904.12012.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=iyT_fkOA2yg">video</a> /
              <a href="data/revealnet.txt">bibtex</a> /
              <a href='http://niessnerlab.org/projects/hou2020revealnet.html'>project</a>
              <p>This paper introduces the task of semantic instance completion: from an incomplete, 
                RGB-D scan of a scene, we detect the individual object instances comprising the scene and jointly infer their complete object geometry.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/3dsis.png" alt="3dsis" width="160" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1904.12012.pdf">
                <papertitle>3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans</papertitle>
              </a>
              <br>
              <strong>Ji Hou</strong>,
              <a href="https://www.3dunderstanding.org/team.html">Angela Dai</a>,
              <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a>
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR), 2019, Long Beach, USA</em> <br>
              <font color="red">(Oral Presentation)</font>
              <br>
              <a href="https://arxiv.org/pdf/1812.07003.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=IH9rNLD1-JE">video</a> /
              <a href="data/3dsis.txt">bibtex</a> /
              <a href='http://niessnerlab.org/projects/hou2019sis.html'>project</a> /
              <a href='https://github.com/Sekunde/3D-SIS'>code</a>
              <p>We introduce 3D-SIS, a novel neural network architecture for 3D semantic instance segmentation in commodity RGB-D scans. The core idea of our method is to jointly learn from both geometric and color signal, thus enabling accurate instance predictions.</p>
            </td>
          </tr>

        </tbody></table>
        
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
		<td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/teaching.png" width="160" style="border-radius:15px">
                </td>
                <td width="75%" valign="center" style="line-height:30px;">
                   <a target="_blank" href="https://www.3dunderstanding.org/teaching.html">Teaching
                    Assistant, Seminar for 3D Machine Learning  - Summer 2021</a>
		  <br>
                  <a target="_blank" href="https://dvl.in.tum.de/teaching/adl4cv-ws19/">Teaching
                    Assistant, Advanced Deep Learning for Computer Vision - Winter 2019/20</a>
                
                  <a target="_blank" href="https://dvl.in.tum.de/teaching/i2dl-ss18/">Teaching
                    Assistant, Introdcution to Deep Learning - Summer 2018</a>
			<br>
                  <a target="_blank" href="https://vision.cs.tum.edu/teaching/ws2017/dl4cv">Teaching
                    Assistant, Deep Learning for Computer Vision - Winter 2017/18</a>
                </td>
              </tr>
            </tbody>
          </table>
        
        
        
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <br>
        <p align="right"><font size="2">
      Credits: <a href="https://people.eecs.berkeley.edu/~barron/" target="_blank">Jon Barron</a>
          </font>
        </p>
        </td>
      </tr>
      </tbody></table>
	      

</body>

</html>
