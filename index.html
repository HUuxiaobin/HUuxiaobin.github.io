<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xiaobin Hu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
  <table style="width:100%;max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiaobin Hu (胡晓彬)</name>
              </p>
              <p> 
                I am a research scientist at Tencent working closely with Dr. Ying Tai and Dr. Chengjie Wang through ‘腾讯技术大咖’ program. I receive Shanghai Overseas Talents Award (Baiyulan Young Talent Program) in 2023,
                Before that, I obtained my Ph.D. degree at the School of Computer Science and Engineering, Technische Universität München, Germany, under the joint supervision of Prof. Bjoern Menze and Prof. Kuangyu Shi.  
    During Phd studies, I also worked as a long-term intern in Chinese Academy of Sciences with Prof. Wenqi Ren, and a half-year internship with Prof. Dengping Fan and Hang Dai in IIAI and Mbzuai of United Arab Emirates. 
              </p>
              <p>
               As a <B> first </B> or <B> corresponding </B> author, I have published over <B> 15 </B> papers on top-tier conferences and journals, such as ICCV, ECCV, TPAMI, MICCAI, JBHI, EJNM etc. (Total Top/Q1 Journal in Chinese Academy of Sciences ranking: 6; Top conference: 4;)
              </p>
		Email: <a href="xbhunanu@gmail.com">xbhunanu [at] gmail.com</a></p>
           </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="./files/photo.jpg"><img style="width:100%;max-width:100%;border-radius:100%" alt="Avatar" src="./files/photo.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
	</tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
		    
	My research interests are mainly at image restoration and neural architecture search.
	Currently, I investigate on how to solve <B> the real-world low-quality image restoration </B> problems. In addition, I also have some experience on the following research topics:</p>
	<ul>
	<li>Image restoration inculding super-resolution and deblurring tasks</li>
	<li>Neural architecture search</li>	
	<li>3D face and related Tasks</li>
	<li>Image to image translation</li>
	<li>Uncertainty analysis and interpretation </li>
	<li>Segmentation, detection and classification</li>
	<li>Domain adaptation</li>
	<li>Medical image analysis</li>
	
<!-- 	<li>GAN Priors</li> -->
<!-- 	<li>Image Inpainting and Completion</li> -->

	</ul>
	  	   
      <p style="text-align:center">
			<a href="mailto:xbhunanu@gmail.com"><i class="fa fa-envelope" style="font-size:20px" target="_blank"></i>&nbsp; Email</a> &nbsp;/&nbsp;
			<a href="https://scholar.google.de/citations?user=3lMuodUAAAAJ&hl=en" target="_blank"><i class="ai ai-google-scholar-square ai-3x" style="font-size:20px"></i>&nbsp; Google Scholar</a> &nbsp;/&nbsp;
			<a href="https://github.com/HUuxiaobin" target="_blank"><i class="fa fa-github" style="font-size:20px"></i>&nbsp; Github</a> &nbsp;/&nbsp;
<!-- 			<a href="https://twitter.com/sekunde_" target="_blank"><i class="fa fa-twitter" style="font-size:20px"></i>&nbsp; Twitter</a> &nbsp;/&nbsp;
			<a href="https://www.linkedin.com/in/ji-hou-3b64a2a0/" target="_blank"><i class="fa fa-linkedin" style="font-size:20px"></i>&nbsp; Linkedin</a>  -->
			</p>	
	         

<!--news -- -->
<h2>News</h2>
<ul>
<!--   <li><p style="text-align:left">07/2024 – We released the website/dataset/codes/models/arxiv of <a href="https://nju-pcalab.github.io/projects/openvid/"; style="color: #EE7F2D;"> <b>OpenVid-1M</b></a> (a high-quality text-to-video dataset to enhance video quality, featuring high aesthetics, clarity, and resolution). </p></li> 
 -->
 <li><p style="text-align:left">07/2024 – 1 paper accepted by MM 2024 </p></li>	
 <li><p style="text-align:left">07/2024 – 2 paper accepted by ECCV 2024 </p></li>
 <li><p style="text-align:left">07/2024 – 1 paper accepted by IEEE Transactions on Circuits and Systems for Video Technology (TCSVT). </p></li>	
<li><p style="text-align:left">07/2024 – 1 paper accepted by Pattern Recognition (PR). </p></li>
</ul>
              <h2>Selected Publications</h2>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	

<!-- ###---------------------------------------------------------------paper diffumatting  -->
	<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/eccv24.png" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf"> -->
                <papertitle>DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation</papertitle>
<!--               </a> -->
              <br>
              <strong>Xiaobin Hu</strong>,
		    Xu Peng, 
		    Donghao Luo, 
		    Xiaozhong Ji, 
		    Jinlong Peng, 
		    Zhengkai Jiang, 
		    Jiangning Zhang, 
		     Taisong Jin,
		    Chengjie Wang, 
		    Rongrong Ji
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em>European Conference on Computer Vision (ECCV), 2024 </em>
              <br>
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
		paper /
              video /
              bibtex /
	      code
              <p> Our DiffuMatting shows several potential applications (e.g., matting-data generator, community-friendly art design and controllable generation). </p>
            </td>
          </tr>
<!-- ###---------------------------------------------------------------TCSVT.png  -->
	<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/TCSVT.png" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf"> -->
                <papertitle>Face Restoration via Plug-and-Play 3D Facial Priors</papertitle>
<!--               </a> -->
              <br>
              Qian Xu*,
		    <strong> Xiaobin Hu </strong>*, 
		    Donghao Luo, 
		    Ying Tai, 
		    Chengjie Wang, 
		    Yuntao Qian, (*equal contribution)
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em>IEEE Transactions on Circuits and Systems for Video Technology, 2024 </em>
              <br>
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
		paper /
              video /
              bibtex /
	      code
              <p> Video deblurring is a challenging task as the blur is often spatially variant. Existing methods mainly engage in building the spatial-temporal correspondence among the frames</p>
            </td>
          </tr>
<!-- ###---------------------------------------------------------------acmmm  -->
	<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/mm.png" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf"> -->
                <papertitle>3D Priors-Guided Diffusion for Blind Face Restoration</papertitle>
<!--               </a> -->
              <br>
              Xiaobin Lu*,
		    <strong> Xiaobin Hu </strong>*, 
		    Jun Luo, 
		    zhuben, 
		    paulruan,
		    Wenqi Ren, (*equal contribution)
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em>ACM Multimedia (MM), 2024 </em>
              <br>
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
		paper /
              video /
              bibtex /
	      code
              <p> A customized multi-level feature extraction method is employed to exploit both structural and identity information of 3D facial images, which are then mapped into the noise estimation process.</p>
            </td>
          </tr>


<!-- ###---------------------------------------------------------------pr -->
	<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/PR.png" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf"> -->
                <papertitle>Joint-Individual Fusion Structure with Fusion Attention Module for Multi-Modal Skin Cancer Classification</papertitle>
<!--               </a> -->
              <br>
Peng Tang, Xintong Yan, Yang Nan, <strong> Xiaobin Hu </strong> #, Bjoern H Menze, Sebastian Krammer, Tobias Lasser. (corresponding author: #)		    
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em>ACM Multimedia (MM), 2024 </em>
              <br>
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
		paper /
              video /
              bibtex /
	      code
              <p> Thus, this paper introduces a novel fusion method that integrates dermatological images (dermoscopy images or clinical images) with patient metadata for skin cancer classification, focusing on enhancing FS and FM components.</p>
            </td>
          </tr>
		
<!-- ###---------------------------------------------------------------medical images  -->
	<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/Quantitative_Imaging_in_Medicine_and_Surger.png" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf"> -->
                <papertitle>Automated segmentation of the human supraclavicular fat depot via deep neural network in water-fat separated magnetic resonance images </papertitle>
<!--               </a> -->
              <br>
Yu Zhao, 
Chunmeng Tang,
Bihao Cui,
Arun Somasundaram, 
Johannes Raspe, 
<strong> Xiaobin Hu </strong>#, 
Christina Holzapfel, Daniela Junker, Hans Hauner, Bjoern Menze, Mingming Wu, Dimitrios Karampinos. (corresponding author # )
		    
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em>Quantitative Imaging in Medicine and Surgery, 2023 </em>
              <br>
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
		paper /
              video /
              bibtex /
	      code
              <p> Human brown adipose tissue (BAT), mostly located in the cervical/supraclavicular region, is a promising target in obesity treatment. Magnetic resonance imaging (MRI) allows for mapping the fat content quantitatively.</p>
            </td>
          </tr>
<!-- ###---------------------------------------------------------------paper 1 TPAMI  -->
	<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/tpami_3d_priors.JPG" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf"> -->
                <papertitle>Face Restoration via Plug-and-Play 3D Facial Priors</papertitle>
<!--               </a> -->
              <br>
              <strong>Xiaobin Hu</strong>,
		    Wenqi Ren, 
		    Jiaolong Yang, 
		    Xiaochun Cao, 
		    David Wipf, 
		    Bjoern Menze, 
		    Xin Tong, 
		     Hongbin Zha,    
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021 </em>
              <br>
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
		paper /
              video /
              bibtex /
	      code
              <p> Existing face restoration algorithms only employ 2D priors without considering high dimensional information (3D). The 3D morphable facial priors are the main novelty of this work and are completely different from recently related 2D prior works</p>
            </td>
          </tr>
		
<!-- ###----------------------------------------------------------------paper line ECCV 2020-->
		<br>
		<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/3d_priors_eccv.JPG" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490732.pdf">
                <papertitle>Face Super-Resolution Guided by 3D Facial Priors</papertitle>
              </a>
              <br>
              <strong>Xiaobin Hu</strong>,
		 Wenqi Ren,
                 John LaMaster,
		Xiaochun Cao,
		Xiaoming Li,
		 Zechao Li,
		 Bjoern Menze,    
		 Wei Liu,   
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em>European Conference on Computer Vision (ECCV), 2020, </em>
              <br>
		    <font color="red">(Spotlight Presentation)</font>
	      <br>
               <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490732.pdf">paper</a> /
              video /
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:7O_TKmvpuxQJ:scholar.google.com/&output=citation&scisdr=CgXSEHgOENLesYiJAF8:AAGBfm0AAAAAYQ-PGF9QmYd8hwLFmS-AJYsXhdOF3c0D&scisig=AAGBfm0AAAAAYQ-PGPibfsXerjWGa5KI5iGr_9MaW83U&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a> /
	      <a href="https://github.com/HUuxiaobin/Face-Super-Resolution-Guided-by-3D-Facial-Priors">code</a> 
<!-- 		paper /
              video /
              bibtex /
	      code -->
              <p> In this paper, we propose a novel face super resolution method that explicitly incorporates 3D facial priors which grasp the sharp facial structures. Our work is the first to explore 3D morphable knowledge based on the fusion of parametric descriptions of face attributes (e.g., identity, facial expression, texture, illumination, and face pose) </p>
            </td>
          </tr>			
		
<!-- %%%------------------------------------------------------paper line 2 ICCV-->
		
		<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/nas_deblurring.JPG" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
<!--               <a href="https://arxiv.org/pdf/2104.11225.pdf"> -->
                <papertitle>Pyramid Architecture Search for Real-Time Image Deblurring</papertitle>
<!--               </a> -->
              <br>
              <strong>Xiaobin Hu</strong>,
		 Wenqi Ren,
                 Kaicheng Yu,
		Kaihao Zhang,
		Xiaochun Cao,
		 Wei Liu,
		 Bjoern Menze,    
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em>International Conference on Computer Vision (ICCV), 2021, Montreal, Canada </em>
              <br>
             <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Hu_Pyramid_Architecture_Search_for_Real-Time_Image_Deblurring_ICCV_2021_paper.html">paper</a> /
<!--               <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
              video /
              bibtex /
	      code
              <p> we propose a novel deblurring method, dubbed PyNAS, towards automatically designing hyper-parameters including the scales, patches, and standard cell operators. Our primary contribution is a real-time deblurring algorithm (around 58 fps) for 720p images while achieves state-of-the-art deblurring performance on the GoPro and Video Deblurring datasets.</p>
            </td>
          </tr>
		
		
<!-- %%%------------------------------------------------------paper line  TIP-->
		
		<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/TIP_2021.JPG" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/9426447">
                <papertitle>SRGAT: Single Image Super-Resolution With Graph Attention Network</papertitle>
              </a>
              <br>
		    Yanyang Yan,
		    Wenqi Ren,
              <strong> Xiaobin Hu</strong>,
		    Kun Li,
                 Haifeng Shen,
		Xiaochun Cao,   
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em> IEEE Transactions on Image Processing (TIP), 2021 </em>
              <br>
              <a href="https://ieeexplore.ieee.org/document/9426447">paper</a> /
<!--               <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
<!-- 		paper / -->
              video /
              bibtex /
	      code
<!--               <p> we propose a novel deblurring method, dubbed PyNAS, towards automatically designing hyper-parameters including the scales, patches, and standard cell operators. Our primary contribution is a real-time deblurring algorithm (around 58 fps) for 720p images while achieves state-of-the-art deblurring performance on the GoPro and Video Deblurring datasets.</p> -->
            </td>
          </tr>		
	
<!-- %%%------------------------------------------------------paper line CVPR 2021-->
		
		<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/CVPR_2021.JPG" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Ultra-High-Definition_Image_Dehazing_via_Multi-Guided_Bilateral_Learning_CVPR_2021_paper.pdf">
                <papertitle>Ultra-High-Definition Image Dehazing via Multi-Guided Bilateral Learning</papertitle>
              </a>
              <br>
               Zhuoran Zheng, 
		Wenqi Ren, 
		Xiaochun Cao, 
              <strong> Xiaobin Hu</strong>,
		    Tao Wang, 
		    Fenglong Song, 
		    Xiuyi Jia,
  
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em> Computer Vision and Pattern Recognition (CVPR), 2021 </em>
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Ultra-High-Definition_Image_Dehazing_via_Multi-Guided_Bilateral_Learning_CVPR_2021_paper.pdf">paper</a> /
<!--               <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
<!-- 		paper / -->
              video /
              bibtex /
	      code
<!--               <p> we propose a novel deblurring method, dubbed PyNAS, towards automatically designing hyper-parameters including the scales, patches, and standard cell operators. Our primary contribution is a real-time deblurring algorithm (around 58 fps) for 720p images while achieves state-of-the-art deblurring performance on the GoPro and Video Deblurring datasets.</p> -->
            </td>
          </tr>		
		
<!-- %%%------------------------------------------------------paper line  medical_european journal-->
		
		<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/european_journal.JPG" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/article/10.1007/s00259-021-05232-3">
                <papertitle>Weakly supervised deep learning for determining the prognostic value of 18 F-FDG PET/CT in extranodal natural killer/T cell lymphoma, nasal type</papertitle>
              </a>
              <br>
		    Rui Guo*, 
              <strong> Xiaobin Hu *</strong>,
		    Haoming Song, 
		    Pengpeng Xu, 
		    Haoping Xu, 
		    Axel Rominger, 
		    Xiaozhu Lin, 
		    Bjoern Menze, 
		    Biao Li, 
		    Kuangyu Shi, (*equal contribution)
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em> European Journal of Nuclear Medicine and Molecular Imaging, 2021, Top journal </em>
              <br>
              <a href="https://link.springer.com/article/10.1007/s00259-021-05232-3">paper</a> /
<!--               <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
<!-- 		paper / -->
              video /
              bibtex /
	      code
		<br>
<!--               <p> we propose a novel deblurring method, dubbed PyNAS, towards automatically designing hyper-parameters including the scales, patches, and standard cell operators. Our primary contribution is a real-time deblurring algorithm (around 58 fps) for 720p images while achieves state-of-the-art deblurring performance on the GoPro and Video Deblurring datasets.</p> -->
            </td>
          </tr>	
	<br>

				<!-- %%%------------------------------------------------------paper line Frition 2021-->
		
		<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/frition.JPG" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
<!--               <a href="https://openreview.net/pdf?id=k1BSWQqHoMV"> -->
                <papertitle>Morphological Residual Convolutional Neural Network (MRCNN) for Intelligent Recognition of Wear Particles From Artificial Joints</papertitle>
<!--               </a> -->
              <br>
              <strong> Xiaobin Hu</strong>,
		 Jian Song, 
		  Zhenhua Liao, 
		    Yuhong Liu, 
		    Jian Gao,
		    Bjoern Menze, 
		    Weiqiang Liu,
  
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em> Friction, 2021, Top journal </em>
              <br>
<!--               <a href="https://openreview.net/pdf?id=k1BSWQqHoMV">paper</a> / -->
<!--               <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
		paper /
              video /
              bibtex /
	      code
<!--               <p> we propose a novel deblurring method, dubbed PyNAS, towards automatically designing hyper-parameters including the scales, patches, and standard cell operators. Our primary contribution is a real-time deblurring algorithm (around 58 fps) for 720p images while achieves state-of-the-art deblurring performance on the GoPro and Video Deblurring datasets.</p> -->
            </td>
          </tr>	
		
		
		
		
		<!-- %%%------------------------------------------------------paper line MIDL 2021-->
		
		<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/midl.JPG" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/pdf?id=k1BSWQqHoMV">
                <papertitle>Feedback Graph Attention Convolutional Network for MR Images Enhancement by Exploring Self-Similarity Features</papertitle>
              </a>
              <br>
              <strong> Xiaobin Hu</strong>,
		    Yanyang Yan,
		    Wenqi Ren,
		    Hongwei Li,
		    Amirhossein Bayat,
		    Yu Zhao,
		    Bjoern Menze,
  
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em> Medical Imaging with Deep Learning (MIDL), 2021 </em>
              <br>
              <a href="https://openreview.net/pdf?id=k1BSWQqHoMV">paper</a> /
<!--               <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
<!-- 		paper / -->
              video /
              bibtex /
	      code
<!--               <p> we propose a novel deblurring method, dubbed PyNAS, towards automatically designing hyper-parameters including the scales, patches, and standard cell operators. Our primary contribution is a real-time deblurring algorithm (around 58 fps) for 720p images while achieves state-of-the-art deblurring performance on the GoPro and Video Deblurring datasets.</p> -->
            </td>
          </tr>	
		
		
		
<!-- %%%------------------------------------------------------paper line  jbhi-->
		
		<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/jbhi_20.JPG" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/8988186?denied=">
                <papertitle>Coarse-to-Fine Adversarial Networks and Zone-Based Uncertainty Analysis for NK/T-Cell Lymphoma Segmentation in CT/PET Images</papertitle>
              </a>
              <br>
              <strong> Xiaobin Hu </strong>,
			 Rui Guo,
		    Jieneng Chen,
		    Hongwei Li,
		    Diana Waldmannstetter,
		    Yu Zhao,
		    Biao Li,
                   Kuangyu Shi,
		    Bjoern Menze,
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em> Journal of Biomedical and Health Informatics, 2020, Top journal </em>
              <br>
              <a href="https://ieeexplore.ieee.org/document/8988186?denied=">paper</a> /
<!--               <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
<!-- 		paper / -->
              video /
              bibtex /
	      code
		<br>
<!--               <p> we propose a novel deblurring method, dubbed PyNAS, towards automatically designing hyper-parameters including the scales, patches, and standard cell operators. Our primary contribution is a real-time deblurring algorithm (around 58 fps) for 720p images while achieves state-of-the-art deblurring performance on the GoPro and Video Deblurring datasets.</p> -->
            </td>
          </tr>	

<!-- %%%------------------------------------------------------paper line  miccai-->
		
		<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/miccai.JPG" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-32226-7.pdf">
                <papertitle>Spatial-Frequency Non-local Convolutional LSTM Network for pRCC Classification</papertitle>
              </a>
              <br>
		    Yu Zhao, et al.,
              <strong> Xiaobin Hu <sup>#</sup> </strong>,
		    Bjoern Menze;  corresponding author <sup>#</sup>.
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2019) </em>
              <br>
              <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-32226-7.pdf">paper</a> /
<!--               <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
<!-- 		paper / -->
              video /
              bibtex /
	      code
		<br>
<!--               <p> we propose a novel deblurring method, dubbed PyNAS, towards automatically designing hyper-parameters including the scales, patches, and standard cell operators. Our primary contribution is a real-time deblurring algorithm (around 58 fps) for 720p images while achieves state-of-the-art deblurring performance on the GoPro and Video Deblurring datasets.</p> -->
            </td>
          </tr>	
		
<!-- %%%------------------------------------------------------paper line  robotic-->
		
		<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="files/robotic.JPG" alt="pri3d" width="300" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.frontiersin.org/articles/10.3389/fnbot.2019.00040/full">
                <papertitle>Toward a Brain-Inspired System: Deep Recurrent Reinforcement Learning for a Simulated Self-Driving Agent</papertitle>
              </a>
              <br>
                     Jieneng Chen, 
		    Jingye Chen, 
		    Ruiming Zhang,
		   <strong> Xiaobin Hu <sup>#</sup> </strong>;  corresponding author <sup>#</sup>.
<!--               <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a> -->
              <br>
		    <em> Frontiers in neurorobotics, 2019 </em>
              <br>
              <a href="https://www.frontiersin.org/articles/10.3389/fnbot.2019.00040/full">paper</a> /
<!--               <a href="https://www.youtube.com/watch?v=l0JxnpbaN38">video</a> /
              <a href="data/pri3d.txt">bibtex</a> /
	      <a href='https://github.com/Sekunde/Pri3D'>code</a> -->
<!-- 		paper / -->
              video /
              bibtex /
	      code
		<br>
<!--               <p> we propose a novel deblurring method, dubbed PyNAS, towards automatically designing hyper-parameters including the scales, patches, and standard cell operators. Our primary contribution is a real-time deblurring algorithm (around 58 fps) for 720p images while achieves state-of-the-art deblurring performance on the GoPro and Video Deblurring datasets.</p> -->
            </td>
          </tr>	
				
<!-- %%%%----------------------------------------------- -->
		
        </tbody></table>
        
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h4>Awards / Honors Challenge</h4>
            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
		<td style="padding:20px;width:18%;vertical-align:middle">
                  <img src="files/award.JPG" width="160" style="border-radius:15px">
                </td>
                <td width="75%" valign="center" style="line-height:30px;">
		<li>China National Scholarship</li>
		<li>Excellent Graduate of Hunan Province</li>	
		<li>Excellent Master Thesis of Hunan Province</li>
		<li>The Winner of the National Postgraduate Mathematical Modeling Competition</li>
		<li>MICCAI Multimodal Brain Tumor Segmentation Challenge 2018 </li>
		<li>LiTS - Liver Tumor Segmentation Challenge 2017</li>
		<li>Quantification of Uncertainties in Biomedical Image Quantification Challenge 2020 </li>
<!--                    <a target="_blank" href="https://www.3dunderstanding.org/teaching.html">Teaching
                    Assistant, Seminar for 3D Machine Learning  - Summer 2021</a>
		  <br>
                  <a target="_blank" href="https://dvl.in.tum.de/teaching/adl4cv-ws19/">Teaching
                    Assistant, Advanced Deep Learning for Computer Vision - Winter 2019/20</a>
                
                  <a target="_blank" href="https://dvl.in.tum.de/teaching/i2dl-ss18/">Teaching
                    Assistant, Introdcution to Deep Learning - Summer 2018</a>
			<br>
                  <a target="_blank" href="https://vision.cs.tum.edu/teaching/ws2017/dl4cv">Teaching
                    Assistant, Deep Learning for Computer Vision - Winter 2017/18</a> -->
                </td>
              </tr>
            </tbody>
          </table>
<!-- %%%%----------------------------------------------- -->		
		

		

   
<!-- <p><B>Awards/Honors Challenge:  </B></p>
<font size="3"> 
<ul>
<li>China National Scholarship</li>
<li>Excellent Graduate of Hunan Province</li>	
<li>Excellent Master Thesis of Hunan Province</li>
<li>The Winner of the National Postgraduate Mathematical Modeling Competition</li>
<li>MICCAI Multimodal Brain Tumor Segmentation Challenge 2018 </li>
<li>LiTS - Liver Tumor Segmentation Challenge 2017</li>
<li>Quantification of Uncertainties in Biomedical Image Quantification Challenge 2020 </li>
</ul>
</font>
<br />
	
<p><B>Journal Reviewer:  </B></p>
<font size="3"> 
<ul>
<li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
<li>IEEE Transactions on Image Processing (TIP)</li>
<li>Computer Vision and Image Understanding (CVIU)</li>
</ul>
</font>
<br />
<p><B>Conference Reviewer: </B></p>
<font size="3"> 
<ul>
<li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
<li>International Joint Conferences on Artificial Intelligence (IJCAI)</li>
<li>Asian Conference on Computer Vision (ACCV)</li>
</ul>
</font> -->



</body>

</html>
